# pytorch-CIFAR10-Resnet

#### 人工智能2101班 22151290 付振东

在pytorch框架下用resnet18在CIFAR10数据集上实现多分类

acc在70%左右

因为resnet加入了残差层，在一定程度上可以缓解梯度消失的问题。而且因为在正向卷积时，每一层做卷积其实只提取了图像的一部分信息，这样一来，越到深层，原始图像信息就会丢失的更严重，所以直接把低层次的信息直接加入到高层次输出，也可以在一定程度上缓解这个问题。

调参：

batch_size:batch_size增加，确实能大幅度的提升模型的训练速度，但是有可能会导致收敛的结果变差。而且增加batch_size，对显存的要求会变高。在收敛到同一acc或者loss的前提下，batch_size越大，需要的epoch越多，但整体的训练速度在batch_size合理的情况下还是更快的。所以得到的结论是，在显存足够的前提下，尽可能的提升batch_size，增加epoch是更好的做法。

dropout:当模型欠拟合时，一般表现出训练集acc很高，但是测试集acc比训练集acc低很多，这种情况多是数据集要比模型更复杂，所以应该适当的增加模型的神经元个数或者神经网络的深度，降低dropout。当模型过拟合时，一般表现为训练集acc一直增加，测试集acc先增加后降低。这种情况一般是模型要比数据集更复杂，可以增加训练集数量，增加dropout概率，加入正则项，来增加模型的泛化能力。

